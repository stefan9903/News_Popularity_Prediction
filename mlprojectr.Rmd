---
title: "R Notebook"
output: html_notebook
---

This is an [R Markdown](http://rmarkdown.rstudio.com) Notebook. When you execute code within the notebook, the results appear beneath the code. 

Try executing this chunk by clicking the *Run* button within the chunk or by placing your cursor inside it and pressing *Ctrl+Shift+Enter*. 

```{r}
plot(cars)
```

Add a new chunk by clicking the *Insert Chunk* button on the toolbar or by pressing *Ctrl+Alt+I*.

When you save the notebook, an HTML file containing the code and output will be saved alongside it (click the *Preview* button or press *Ctrl+Shift+K* to preview the HTML file).

The preview shows you a rendered HTML copy of the contents of the editor. Consequently, unlike *Knit*, *Preview* does not run any R code chunks. Instead, the output of the chunk when it was last run in the editor is displayed.
```{r}
pacman::p_load(dplyr, recipes, caret, ranger)
```
```{r}
news <- read.csv("C:\\Users\\lenovo\\Desktop\\OnlineNewsPopularity_label.csv")
news <- news[, c(1:37, 59)]

head(news)
```

```{r}
set.seed(7027)  # for reproducibility
split <- rsample::initial_split(news, prop = 0.7, strata = "shares")
news_train <- rsample::training(split)
news_test <- rsample::testing(split)
```

```{r}
library(recipes)
xgb_prep<-recipe(shares~.,data=news_train)%>%
  step_nzv(all_nominal())%>%
  step_integer(matches("Qual|Cond|QC|Qu"))%>%
  step_center(all_numeric(),-all_outcomes())%>%
  step_scale(all_numeric(),-all_outcomes())%>%
  step_dummy(all_nominal())%>%
  prep(training=news_train,retain=TRUE)%>%
  juice()
```

```{r}
X<-as.matrix(xgb_prep[setdiff(names(xgb_prep),"shares")])
Y<-xgb_prep$shares
```

```{r}
library(xgboost)
set.seed(7027)
data_xgb<-xgb.cv(
  data=X,
  label=Y,
  nrounds=200,
  objective="reg:squarederror",
  early_stopping_rounds=50,
  nfold=10,
  params=list(
    eta=0.01,
    max_depth=3,
    subsample=0.5,
    colsample_bytree=0.5),
  verbose=0
  )
```

```{r}
min(data_xgb$evaluation_log$test_rmse_mean)
```

```{r}
hyper_grid3<-expand.grid(
  eta=0.01,
  max_depth=3,
  min_child_weight=3,
  subsample=0.5,
  colsample_bytree=0.5,
  gamma=c(0,1,10,100,1000),
  lambda=c(0,0.01,0.1,1,100,1000,10000),
  alpha=c(0,0.01,0.1,1,100,1000,10000),
  rmse=0,
  trees=0
)
```

```{r}
for (i in seq_len(nrow(hyper_grid3))){
  set.seed(87)
  m1<-xgb.cv(
    data=X,
    label=Y,
    nrounds=500,
    objective="reg:squarederror",
    early_stopping_rounds=50,
    nfold=10,
    verbose=0,
    params=list(
      eta=hyper_grid3$eta[i],
      max_depth=hyper_grid3$max_depth[i],
      min_child_weight=hyper_grid3$min_child_weight[i],
      subsample=hyper_grid3$subsample[i],
      colsample_bytree=hyper_grid3$colsample_bytree[i],
      gamma=hyper_grid3$gamma[i],
      lambda=hyper_grid3$lambda[i],
      alpha=hyper_grid3$alpha[i]
    )
  )
  hyper_grid3$rmse[i]<-min(m1$evaluation_log$test_rmse_mean)
  hyper_grid3$trees[i]<-m1$best_iteration
}
```

```{r}
n_features<-length(setdiff(names(news_train),"shares"))
```
```{r}
library(ranger)
data_rfp<-ranger(shares~.,data=news_train,mtry=floor(n_features/3),seed=7027)
```
```{r}
default_rmse<-sqrt(data_rfp$prediction.error)
default_rmse
```

```{r}
library(dplyr)

hyper_grid3%>%
  arrange(rmse)%>%
  mutate(perc_gain=(default_rmse-rmse)/default_rmse*100)%>%
  head(4)
```
```{r}
bst <- xgboost(data=X,
    label=Y,
    nrounds=5,
    objective="reg:squarederror",
    early_stopping_rounds=50,
    nfold=5,
    verbose=0,
    params=list(
      eta=0.01,
      max_depth=3,
      min_child_weight=3,
      subsample=0.5,
      colsample_bytree=0.5,
      gamma=0,
      lambda=100,
      alpha=0.1))

```

```{r}
vip::vip(bst,scale=TRUE)
```

